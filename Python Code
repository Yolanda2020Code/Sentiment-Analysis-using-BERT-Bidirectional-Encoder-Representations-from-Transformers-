import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from transformers import glue_convert_examples_to_features, glue_processors

# Load tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')

# Sample data for demonstration
texts = ["I love this movie!", "This was a waste of time."]
labels = [1, 0]  # 1 for positive, 0 for negative

# Tokenize inputs
input_ids = []
attention_masks = []

for text in texts:
    encoded_dict = tokenizer.encode_plus(text, add_special_tokens=True, max_length=64, pad_to_max_length=True, return_attention_mask=True)
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = np.array(input_ids)
attention_masks = np.array(attention_masks)

# Convert to TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_masks, labels))

# Compile and train model
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

model.fit(train_dataset.batch(1), epochs=3)

# Save model
model.save_pretrained('sentiment_bert')
